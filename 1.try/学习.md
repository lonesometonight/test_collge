# lof算法

## 1、导入库

1. ```
   import numpy as np
   import matplotlib.pyplot as plt
   from sklearn.neighbors import LocalOutlierFactor
   ```

## 2、导入数据——常见的随机函数

### 2.1.1 随机固定

``np.random.seed(42)``

让每一次随机产生的变量都是固定的

### 2.1.2 返回100*2的二维数组

``np.random.randn(100, 2)``

numpy.random.randn(d0, d1, …, dn)是从标准正态分布中返回一个或多个样本值。
numpy.random.rand(d0, d1, …, dn)的随机样本位于[0, 1)中。

### 2.1.3 拼接数组 按列拼装 上一个加上2，下一个减去2

``X_inliers = np.r_[X_inliers + 2, X_inliers - 2]``
numpy.c_() and numpy.r_()的用法
np.r_是按列连接两个矩阵，就是把两矩阵上下相加，要求列数相等。

np.c_是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等。

### 2.1.4 随机产生符合均匀分布的矩阵20*2

```
     X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))
```

### 2.1.5 产生全是1的一维数组

```
     ground_truth = np.ones(len(X), dtype=int)
```

### 2.1.6 把后面n——outliers个变成-1

```
 ground_truth[-n_outliers:] = -1
```

## 3、训练模型

### 3.1、设置k（n_neighbors）和异常点的比例（contamination）

    ``    clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)``

### 3.2、利用模型来进行预测，如果不是异常值返回1，如果是异常值返回-1

    ``y_pred = clf.fit_predict(X)``

### 3.3如果预测的值和原来的值不想等，统计不想等的个数

    ``n_errors = (y_pred != ground_truth).sum()``

### 3.4、lof的一个方法

```
 negative_outlier_factor_ : numpy array, shape (n_samples,)
```

    和LOF相反的值，值越小，越有可能是异常点。（注：上面提到LOF的值越接近1，越可能是正常样本，LOF的值越大于1，则越可能是异常样本）。这里就正好反一下。

    这个方法返回的是对于这个点的一个错误率，越靠近正负1，表示正常，过大或者过小都是异常值

    ``    X_scores = clf.negative_outlier_factor_``

## 4、绘图

### 4.1 散点图

 **x，y** ：长度相同的数组，也就是我们即将绘制散点图的数据点，输入数据。

 **s** ：点的大小，默认 20，也可以是个数组，数组每个参数为对应点的大小。

 **c** ：点的颜色，默认蓝色 'b'，也可以是个 RGB 或 RGBA 二维行数组。

 **marker** ：点的样式，默认小圆圈 'o'。
 edgecolors：：颜色或颜色序列，默认为 'face'，可选值有 'face', 'none', None。这个是外边那个园的颜色
 alpha=0.5 # 设置透明度

```
X[:, 0]
```

切片器，x为220*2的数组，去每一个的第一个元素组成数组

### 4.2 绘制半径与离群值成正比的圆

```
radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())
```

### 4.3、label的应用

plt.title("Local Outlier Factor (LOF)")

radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())

plt.scatter(X[:, 0], X[:, 1], s=1000 * radius, edgecolors='r',

facecolors='none', label='Outlier scores')

![img](image/学习/1644988268075.png)
plt.axis('tight')坐标轴的限制，可以设置无坐标轴
plt.xlim((-5, 5))x轴的初始与结束
plt.ylim((-5, 5))
plt.xlabel("prediction errors: %d" % (n_errors))x轴的名字
legend = plt.legend(loc='upper left')上面label的摆放位置设置图例大小一致

legend.legendHandles[0]._sizes = [10]
legend.legendHandles[1]._sizes = [20]

# 孤立森林算法

## 1、导入库

```
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
```

## 2、产生数据

### 2.1、伪随机数，让每一次产生的随机数都是一样的

```
rng = np.random.RandomState(42)
```

### 2.2、生成训练数据

x是200*2的数组，然后又按照列拼接起来

* numpy.random.randn(d0, d1, …, dn)是从标准正态分布中返回一个或多个样本值。

```
X = 0.3 * rng.randn(100, 2)
X_train = np.r_[X + 2, X - 2]
```

* 产生一些常规的观察值

  ```
  X = 0.3 * rng.randn(20, 2)
  X_test = np.r_[X + 2, X - 2]
  ```
* 产生一些异常的观察结果，均匀分布

  ```
  X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))
  ```

## 3、训练模型

### 3.1、设置模型的参数

```
clf = IsolationForest(max_samples=100,
                      random_state=rng, contamination='auto')
```

* 自我的理解
  树：也就是随机从训练集里面取x个样本，然后开始二叉树分，然后要么就是分到左右只有一个样本，要么就是到达了指定的层数
  森林：多次重复上面过程
* 参数的意思

  n_estimators：孤立树的个数，默认是100个；*构建多少个itree*，就是一共重复多少次的意思

  max_samples：从测试集中抽取的样本个数，这些样本会训练每一个孤立树，默认值是“auto”意思是最多的样本数不超过256个

  contamination：数据集中异常数据的比例

  max_features=1.0：测试集样本中纳入孤立树进行计算的特征个数

  bootstrap=False：是否有放回取样，false是无放回取样，true是独立树从测试集中有放回取样

### 3.2、训练模型

```
  clf.fit(X_train)
```

### 3.3、进行预测

```
  y_pred_train = clf.predict(X_train)
  y_pred_test = clf.predict(X_test)
  y_pred_outliers = clf.predict(X_outliers)
```

  几个主要函数介绍:

* fit(X)： Fit estimator.（无监督）
* predict(X)：返回值：+1 表示正常样本， -1表示异常样本。
* decision_function(X)：返回样本的异常评分。 值越小表示越有可能是异常样本。

## 4、可视化

### **4.1、numpy.meshgrid()** ——生成 **网格点坐标矩阵** 。

[(87条消息) numpy.meshgrid()理解_lllxxq141592654的博客-CSDN博客_np.meshgrid函数](https://blog.csdn.net/lllxxq141592654/article/details/81532855?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164594225516781683974288%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=164594225516781683974288&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-81532855.pc_search_result_cache&utm_term=np.meshgrid&spm=1018.2226.3001.4187)

```
xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))
```

### 4.2、数据降维

[numpy](https://so.csdn.net/so/search?q=numpy&spm=1001.2101.3001.7020)中的ravel()、flatten()、squeeze()都有将多维数组转换为一维数组的功能，区别：
ravel()：如果没有必要，不会产生源[数据](https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE&spm=1001.2101.3001.7020)的副本
flatten()：返回源数据的副本
squeeze()：只能对维数为1的[维度](https://so.csdn.net/so/search?q=%E7%BB%B4%E5%BA%A6&spm=1001.2101.3001.7020)降维

```
plt.title("IsolationForest")

plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)
 
b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white',
                 s=20, edgecolor='k')
b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green',
                 s=20, edgecolor='k')
c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red',
                s=20, edgecolor='k')
plt.axis('tight')
plt.xlim((-5, 5))
plt.ylim((-5, 5))
plt.legend([b1, b2, c],
           ["training observations",
            "new regular observations", "new abnormal observations"],
           loc="upper left")
plt.show()
```

这些是为了画等值线

```
xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
 

plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)

```

没有他们图像如下

![](image/学习/1645944978579.png)

加上之后，他的图形如下

![](image/学习/1645945231639.png)
